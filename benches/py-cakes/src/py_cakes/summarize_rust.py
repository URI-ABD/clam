"""Summarize the results from the Rust implementation."""

import math
import pathlib

import pandas
import matplotlib.pyplot as plt
import numpy

from . import utils

logger = utils.configure_logger(__name__, "INFO")


ALG_MARKERS_COLORS = {
    "KnnLinear": ("+", "tab:blue"),
    "KnnRepeatedRnn": ("s", "tab:orange"),
    "KnnDepthFirst": ("x", "tab:green"),
    "KnnBreadthFirst": ("o", "tab:red"),
    "RnnLinear": ("v", "tab:purple"),
    "RnnClustered": ("<", "tab:brown"),
}

COMP_COLOR_MARKERS = {
    "hnsw": ("*", "tab:pink"),
    "annoy": ("p", "tab:olive"),
    "faiss-ivf": ("d", "tab:cyan"),
}


CLUSTER_MARKERS_COLORS = {
    "Ball": ("s", "tab:blue"),
    "BalancedBall": ("o", "tab:green"),
    "PermutedBall": ("x", "tab:orange"),
    "PermutedBalancedBall": ("+", "tab:red"),
}


def summarize_rust(
    inp_dir: pathlib.Path,
    out_dir: pathlib.Path,
) -> None:
    """Summarize the results from the Rust implementation.

    The input directory should contain the output files generated by the Rust
    implementation of the CAKES search algorithm. The output directory will
    store the collected results in a CSV file.
    """

    datasets: dict[str, Dataset] = {}
    ball_csv_lists: dict[str, list[pathlib.Path]] = {}

    # Collect all csv files in the input directory
    csv_files = sorted(inp_dir.glob("*.csv"))
    ball_csv_paths = list(filter(lambda f: f.name.endswith("ball.csv"), csv_files))
    bench_csv_files = filter(lambda f: not f.name.endswith("ball.csv"), csv_files)
    for f in bench_csv_files:
        logger.info(f"Processing {f.name}")
        # The names are in the format "<dataset>_<cardinality>_<dimensionality>_<metric>.csv"
        dataset_name, cardinality_str, dimensionality_str, metric = f.stem.split("_")
        cardinality = int(cardinality_str)
        dimensionality = int(dimensionality_str)

        # If the `dataset` ends in a number, it has been synthetically augmented
        # for scalability testing.
        is_augmented = dataset_name[-1].isdigit()
        if is_augmented:
            parts = dataset_name.split("-")
            multiplier = 2 ** int(parts[-1])
            dataset_name = "-".join(parts[:-1])
        else:
            multiplier = 1

        if dataset_name not in datasets:
            datasets[dataset_name] = Dataset(
                name=dataset_name,
                cardinality=cardinality,
                dimensionality=dimensionality,
                metric=metric,
                multiplier=multiplier,
            )
        datasets[dataset_name].add_csv_path(cardinality, f)
        ball_csv_lists[dataset_name] = list(
            filter(lambda x: x.stem.startswith(dataset_name), ball_csv_paths)
        )

    for dataset in datasets.values():
        dataset.summarize_results(out_dir)

    for dataset_name, paths in ball_csv_lists.items():
        # if "fashion" not in dataset_name:
        #     continue
        logger.info(f"Making plots for {dataset_name}")
        paths = list(filter(lambda x: "balanced" not in x.stem, paths))
        paths = list(filter(lambda x: "permuted" not in x.stem, paths))
        keyed_paths = [
            (int(path.stem[len(dataset_name) + 1 :].split("-")[0]), path)
            for path in paths
        ]
        keyed_paths.sort(key=lambda x: x[0])
        paths = [path for _, path in keyed_paths]

        all_props = [
            ("lfd", "LFD"),
            ("radius", "Radius"),
            ("fractal_density", "Fractal Density"),
        ]
        for prop, alias in all_props[1:]:
            plot_prop_percentiles(
                out_dir=out_dir,
                dataset=dataset_name,
                ball_csv_path=paths[0],
                prop=prop,
                prop_alias=alias,
            )

    logger.info("Done.")


class Dataset:
    def __init__(
        self,
        *,
        name: str,
        cardinality: int,
        dimensionality: int,
        metric: str,
        multiplier: int,
    ):
        self.name = name
        self.base_cardinality = cardinality // multiplier
        self.dimensionality = dimensionality
        self.metric = metric
        self.__csv_paths: dict[int, pathlib.Path] = {}

    def __repr__(self):
        parts = [
            f"name={self.name}",
            f"cardinality={self.base_cardinality}",
            f"dimensionality={self.dimensionality}",
            f"metric={self.metric}",
        ]
        return f"Dataset({', '.join(parts)})"

    def __hash__(self):
        return hash(self.name)

    def add_csv_path(self, cardinality: int, path: pathlib.Path) -> None:
        self.__csv_paths[cardinality] = path

    @property
    def csv_paths(self) -> list[tuple[int, pathlib.Path]]:
        paths = list(self.__csv_paths.items())
        paths.sort(key=lambda x: x[0])
        return paths

    def summarize_results(self, out_dir: pathlib.Path) -> None:
        csv_paths = self.csv_paths

        if len(csv_paths) == 0:
            logger.warning(f"  No CSV files found for {self.name}")
            return

        columns = list(pandas.read_csv(csv_paths[0][1]).columns)
        if "mean_distance_computations" not in columns:
            columns.append("mean_distance_computations")

        columns.insert(0, "cardinality")
        out_df = pandas.DataFrame(columns=columns)
        for i, (cardinality, path) in enumerate(csv_paths):
            logger.info(f"  Reading {path.name}")
            df = pandas.read_csv(path)

            if i > 4:
                # Remove any rows whose "algorithm" column is "KnnLinear"
                df = df[df["algorithm"] != "KnnLinear"]
                df.reset_index(drop=True, inplace=True)

            # insert a column with the multiplier
            df.insert(0, "cardinality", cardinality)

            # If there is no "mean_distance_computations" column, add it with a value of 0
            if "mean_distance_computations" not in df.columns:
                df.insert(0, "mean_distance_computations", 0)

            # join the dataframes
            out_df = pandas.concat([out_df, df], ignore_index=True)

        # Change the "throughput" column to floats
        out_df["throughput"] = out_df["throughput"].astype(float)
        # Drop all rows where the k is a nan
        out_df = out_df.dropna(subset=["k"])
        # Change the "k" column to integers
        out_df["k"] = out_df["k"].astype(int)

        # Sort the dataframe by the "cardinality", "cluster" and "algorithm" columns
        out_df.sort_values(
            by=["k", "cardinality", "cluster", "algorithm"], inplace=True
        )
        # Drop the "radius" column
        out_df.drop(columns=["radius"], inplace=True)

        # Reset the index
        out_df.reset_index(drop=True, inplace=True)
        # Save the dataframe to a CSV file
        out_df.to_csv(out_dir / f"{self.name}_{self.metric}.csv", index=False)

        # Get the subset of the dataframe where the "algorithm" column is "KnnLinear"
        # And "k" is 10.
        knn_linear_10 = out_df[
            (out_df["algorithm"] == "KnnLinear") & (out_df["k"] == 10)
        ]
        # Set k to "100" in this subset
        knn_linear_10["k"] = 100
        # Append this subset to the dataframe
        out_df = pandas.concat([out_df, knn_linear_10], ignore_index=True)

        # Sort the dataframe by the "cluster" and "algorithm" columns
        out_df.sort_values(by=["cluster", "algorithm"], inplace=True)
        # Reset the index
        out_df.reset_index(drop=True, inplace=True)

        # Group by the "cluster" column and plot the throughput
        grouped = out_df.groupby(["cluster", "k"])
        for (cluster, k), group in grouped:
            plot_throughput(
                out_dir=out_dir,
                dataset=self.name,
                cluster=cluster,
                k=k,
                group=group,
            )

        # If any "mean_distance_computations" value is non-zero, group by the
        # "algorithm" column and plot the distance counts
        if (out_df["mean_distance_computations"] > 0).any():
            grouped = out_df.groupby(["algorithm", "k"])
            for (algorithm, k), group in grouped:
                plot_distance_counts(
                    out_dir=out_dir,
                    dataset=self.name,
                    algorithm=algorithm,
                    k=k,
                    group=group,
                )


def plot_throughput(
    *,
    out_dir: pathlib.Path,
    dataset: str,
    cluster: str,
    k: int,
    group: pandas.DataFrame,
) -> None:
    """Plot the throughput of all algorithms."""
    title = f"{dataset} - {cluster} - {k=}"
    logger.info(f"  Plotting Throughput {title}")

    # Create a figure and axis
    fig: plt.Figure
    ax: plt.Axes
    m = 0.8
    fig, ax = plt.subplots(figsize=(6 * m, 6 * m))

    # Group by the "algorithm" column
    for alg, g in group.groupby("algorithm"):
        # Sort the rows by the "cardinality" column
        g.sort_values("cardinality", inplace=True)
        # Plot the "cardinality" column on the x-axis against the "throughput"
        # column on the y-axis
        marker, color = ALG_MARKERS_COLORS[alg]
        ax.plot(
            g["cardinality"],
            g["throughput"],
            label=f"{alg}",
            marker=marker,
            color=color,
        )

    min_throughput = float(group["throughput"].min())
    max_throughput = float(group["throughput"].max())

    # If the dataset is one of the ann-benchmarks datasets, we need to add
    # results of HNSW, ANNOY, and FAISS-IVF.
    if not ("silva" in dataset or "radio" in dataset):
        comp_dir = pathlib.Path(__file__).parent / "competitors"
        assert comp_dir.exists(), f"{comp_dir} does not exist"
        assert comp_dir.is_dir(), f"{comp_dir} is not a directory"
        comp_paths = [
            comp_dir / f"{name}.csv" for name in ["hnsw", "annoy", "faiss-ivf"]
        ]
        assert all(p.exists() for p in comp_paths), f"Missing files: {comp_paths}"
        for path in comp_paths:
            comp_df = pandas.read_csv(path)
            # filter for the current dataset and k
            comp_df = comp_df[(comp_df["dataset"] == dataset) & (comp_df["k"] == k)]
            alg = path.stem
            marker, color = COMP_COLOR_MARKERS[alg]
            ax.plot(
                comp_df["cardinality"],
                comp_df["throughput"],
                label=alg.upper(),
                marker=marker,
                color=color,
            )

            max_throughput = max(max_throughput, comp_df["throughput"].max())
            min_throughput = min(min_throughput, comp_df["throughput"].min())

            # if "recall" is in the columns, add it as a floating annotation
            # above each point, but with a smaller font size
            if "recall" in comp_df.columns:
                for _, row in comp_df.iterrows():
                    ax.annotate(
                        f"{row['recall']:.2f}",
                        (row["cardinality"], row["throughput"]),
                        textcoords="offset points",
                        xytext=(0, 6),
                        ha="center",
                        fontsize=8,
                    )

    # Set a good y-axis limit
    y_min = (10 ** math.floor(math.log10(min_throughput))) * 0.9
    y_max = (10 ** math.ceil(math.log10(max_throughput))) * 1.1
    ax.set_ylim(y_min, y_max)

    # Set the title and labels
    # ax.set_title(title)
    ax.set_xlabel("Cardinality")
    ax.set_ylabel("Queries per second")

    # Make the top and right spines invisible
    ax.spines["top"].set_visible(False)
    ax.spines["right"].set_visible(False)

    # Set both axes to a logarithmic scale
    ax.set_xscale("log")
    ax.set_yscale("log")

    # Shrink y-axis by 20%
    box = ax.get_position()
    ax.set_position([box.x0, box.y0, box.width, box.height * 0.8])

    # Put a legend above the plot
    ax.legend(
        loc="upper center",
        bbox_to_anchor=(0.49, 1.45),
        fancybox=True,
        ncol=3,
    )

    # # Tighten the layout
    # plt.tight_layout()

    # Save the figure
    (out_dir / "plots").mkdir(parents=False, exist_ok=True)
    fig.savefig(out_dir / "plots" / f"{dataset}_{cluster}_{k}_throughput.png", dpi=300)

    # Close the figure
    plt.close(fig)


def plot_distance_counts(
    *,
    out_dir: pathlib.Path,
    dataset: str,
    algorithm: str,
    k: int,
    group: pandas.DataFrame,
) -> None:
    """Plot the number of distance computations of all clusters."""
    title = f"{dataset} - {algorithm} - {k=}"
    logger.info(f"  Plotting Distance Counts {title}")

    # Create a figure and axis
    fig: plt.Figure
    ax: plt.Axes
    m = 0.8
    fig, ax = plt.subplots(figsize=(6 * m, 6 * m))

    # Group by the "cluster" column
    for cluster, g in group.groupby("cluster"):
        # Sort the rows by the "cardinality" column
        g.sort_values("cardinality", inplace=True)
        # Plot the "cardinality" column on the x-axis against the
        # "mean_distance_computations" column on the y-axis
        marker, color = CLUSTER_MARKERS_COLORS[cluster]
        ax.plot(
            g["cardinality"],
            g["mean_distance_computations"],
            label=f"{cluster}",
            marker=marker,
            color=color,
        )

    # Set the title and labels
    ax.set_xlabel("Cardinality")
    ax.set_ylabel("# Distance Computations")

    # Make the top and right spines invisible
    ax.spines["top"].set_visible(False)
    ax.spines["right"].set_visible(False)

    # Set both axes to a logarithmic scale
    ax.set_xscale("log")
    ax.set_yscale("log")

    # Shrink y-axis by 20%
    box = ax.get_position()
    ax.set_position([box.x0, box.y0, box.width, box.height * 0.8])

    # Put a legend above the plot
    ax.legend(
        loc="upper center",
        bbox_to_anchor=(0.49, 1.45),
        fancybox=True,
        ncol=2,
    )

    # # Tighten the layout
    # plt.tight_layout()

    # Save the figure
    out_path = out_dir / "distance_counts" / f"{dataset}_{algorithm}_{k}_counts.png"
    out_path.parent.mkdir(parents=False, exist_ok=True)
    fig.savefig(out_path, dpi=300)

    # Close the figure
    plt.close(fig)

    # Plot the throughput of all algorithms.
    fig, ax = plt.subplots(figsize=(6 * m, 6 * m))

    # Group by the "cluster" column
    for cluster, g in group.groupby("cluster"):
        # Sort the rows by the "cardinality" column
        g.sort_values("cardinality", inplace=True)
        # Plot the "cardinality" column on the x-axis against the
        # "mean_distance_computations" column on the y-axis
        marker, color = CLUSTER_MARKERS_COLORS[cluster]
        ax.plot(
            g["cardinality"],
            g["throughput"],
            label=f"{cluster}",
            marker=marker,
            color=color,
        )

    # Set the title and labels
    ax.set_xlabel("Cardinality")
    ax.set_ylabel("Queries Per Second")

    # Make the top and right spines invisible
    ax.spines["top"].set_visible(False)
    ax.spines["right"].set_visible(False)

    # Set both axes to a logarithmic scale
    ax.set_xscale("log")
    ax.set_yscale("log")

    # Shrink y-axis by 20%
    box = ax.get_position()
    ax.set_position([box.x0, box.y0, box.width, box.height * 0.8])

    # Put a legend above the plot
    ax.legend(
        loc="upper center",
        bbox_to_anchor=(0.49, 1.45),
        fancybox=True,
        ncol=2,
    )

    # # Tighten the layout
    # plt.tight_layout()

    # Save the figure
    out_path = out_path.parent / f"{dataset}_{algorithm}_{k}_throughput.png"
    fig.savefig(out_path, dpi=300)

    # Close the figure
    plt.close(fig)


def plot_prop_percentiles(
    *,
    out_dir: pathlib.Path,
    dataset: str,
    ball_csv_path: pathlib.Path,
    prop: str,
    prop_alias: str,
):
    logger.info(
        f"  Plotting {prop_alias} percentiles {dataset = } with ball = {ball_csv_path.name}"
    )

    col_tuples = [
        ("minimum", 0, "tab:pink", "dotted", 0.2 * 2),
        (" 5th percentile", 5, "tab:brown", "dashed", 0.3 * 2),
        ("25th percentile", 25, "tab:purple", "solid", 0.4 * 2),
        ("median", 50, "tab:red", "solid", 0.5 * 2),
        ("75th percentile", 75, "tab:green", "solid", 0.4 * 2),
        ("95th percentile", 95, "tab:orange", "dashed", 0.3 * 2),
        ("maximum", 100, "tab:blue", "dotted", 0.2 * 2),
    ]
    columns = [t[0] for t in col_tuples]
    percentiles = [t[1] for t in col_tuples]
    colors = [t[2] for t in col_tuples]
    styles = [t[3] for t in col_tuples]
    widths = [t[4] for t in col_tuples]
    shades_alphas = [
        ("blue", 0.05 * 2),
        ("orange", 0.1 * 2),
        ("green", 0.2 * 2),
        ("green", 0.2 * 2),
        ("orange", 0.1 * 2),
        ("blue", 0.05 * 2),
    ]

    # Read the CSV file and manipulate the data as needed for specific properties
    inp_df = pandas.read_csv(ball_csv_path)
    if prop in ["radius", "fractal_density"]:
        # Find the largest radius in the dataset
        max_radius = inp_df["radius"].max()
        # Divide all radii by the largest radius
        inp_df["radius"] /= max_radius

    if prop == "fractal_density":
        # If the radius is smaller than f32::epsilon, remove the row
        inp_df = inp_df[inp_df["radius"] > 1e-6]

        # Fractal density is defined as cardinality / (radius) ^ lfd
        # Add a new column for fractal density
        inp_df["fractal_density"] = inp_df["cardinality"] / (
            inp_df["radius"] ** inp_df["lfd"]
        )

    # We will make a new dataframe in which each row is one depth level in the
    # ball tree and the columns are the percentiles of the property values for
    # that depth level.
    prop_df = pandas.DataFrame(columns=columns)

    # Group by the "depth" column
    for depth, group in inp_df.groupby("depth"):
        if depth > 100:
            continue

        prop_values = []
        # Each value must be repeated a number of times equal to the cardinality
        # of the corresponding cluster
        for _, row in group.iterrows():
            prop_values.extend([row[prop]] * row["cardinality"])

        # Calculate the percentile values
        percentile_values = list(map(float, numpy.percentile(prop_values, percentiles)))
        # Add them to the dataframe
        prop_df.loc[depth] = percentile_values

    logger.info(f"  Created {prop} dataframe with {prop_df.shape[0]} rows")
    logger.info(f"  {prop_df.head(10)}")

    # Create a figure and axis
    fig: plt.Figure
    ax: plt.Axes
    m = 0.8
    fig, ax = plt.subplots(figsize=(6 * m, 6 * m))

    y_min = prop_df.max().max()
    y_max = prop_df.min().min()

    # Plot the percentiles
    x = prop_df.index
    for i, col in enumerate(columns):
        y = prop_df[col]
        y_min = min(y_min, y.min())
        y_max = max(y_max, y.max())
        ax.plot(
            x, y, label=col, color=colors[i], linestyle=styles[i], linewidth=widths[i]
        )

    # Shade the are between each pair of percentiles
    for y_lower, y_upper, (color, alpha) in zip(
        columns[:-1], columns[1:], shades_alphas
    ):
        ax.fill_between(
            prop_df.index,
            prop_df[y_lower],
            prop_df[y_upper],
            color=color,
            alpha=alpha,
        )

    # Set the title and labels
    ax.set_xlabel("Depth")
    ax.set_ylabel(prop_alias)

    if prop == "lfd":
        max_lfd = 21 if "random" in dataset else 13
        # Set the y-axis limit to (-1, max_lfd)
        ax.set_ylim(-1, max_lfd)
        # Set the y-ticks to [0, 2, 4, ..., max_lfd]
        y_ticks = numpy.arange(0, max_lfd, 2)
        ax.set_yticks(y_ticks)
        # Add a horizontal line at each y-tick
        for y in y_ticks:
            ax.axhline(y, color="gray", linestyle="solid", linewidth=0.1)
    elif prop == "radius":
        if "silva" in dataset or "radio" in dataset:
            # Set the y-axis to a logarithmic scale
            ax.set_yscale("log")
        else:
            # Add a horizontal line at each of [0.0, 0.2, ..., 1.0]
            y_ticks = [r / 10 for r in range(0, 11, 2)]
            ax.set_yticks(y_ticks)
            for y in y_ticks:
                ax.axhline(y, color="gray", linestyle="solid", linewidth=0.1)
    elif prop == "fractal_density":
        # Set the y-axis to a logarithmic scale
        ax.set_yscale("log")

        # Set a good y-axis limit
        y_min = (10 ** math.floor(math.log10(y_min))) * 0.9
        y_max = (10 ** math.ceil(math.log10(y_max))) * 1.1
        ax.set_ylim(y_min, y_max)

    # Make the top and right spines invisible
    ax.spines["top"].set_visible(False)
    ax.spines["right"].set_visible(False)

    # Shrink y-axis by 20%
    box = ax.get_position()
    ax.set_position([box.x0, box.y0, box.width, box.height * 0.8])

    # Put a legend above the plot
    handles, labels = plt.gca().get_legend_handles_labels()
    order = [0, 3, 6, 1, 5, 2, 4]
    ax.legend(
        [handles[idx] for idx in order],
        [labels[idx] for idx in order],
        loc="upper center",
        bbox_to_anchor=(0.49, 1.45),
        fancybox=True,
        ncol=3,
    )

    # # Tighten the layout
    # plt.tight_layout()

    # Save the figure
    out_path = out_dir / prop / f"{dataset}.png"
    out_path.parent.mkdir(parents=False, exist_ok=True)
    logger.info(f"  Saving to {out_path}")
    fig.savefig(out_path, dpi=300)

    # Close the figure
    plt.close(fig)
